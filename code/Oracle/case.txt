Initializing the optimizer...
Number of batches per epoch: 95429
Total number of batches: 954290
EJEMPLO: 
{'lxmertout': tensor([[[ 0.9014, -0.8185,  0.3407,  ..., -0.3601, -0.8549, -0.9188],
         [ 0.7208, -0.9143, -0.0701,  ..., -0.8926,  0.5427, -0.7081]]]), 'sizes': tensor([2]), 'answers': tensor([[1, 1]]), 'game_ids': tensor([2416]), 'raw_q': [['Is it a person?'], ['Is the person a boy?']], 'qids': [tensor([4967]), tensor([4969])]}
torch.Size([1, 2, 768])
ANSWERS: 

tensor([[1, 1]])
Entrada: 
tensor([[[ 0.9014, -0.8185,  0.3407,  ..., -0.3601, -0.8549, -0.9188],
         [ 0.7208, -0.9143, -0.0701,  ..., -0.8926,  0.5427, -0.7081]]],
       device='cuda:0')
Se lo damos de comer a la LSTM
Output LSTM, para cada encoding tenemos su hidden state: 
tensor([[[-7.6159e-01,  0.0000e+00, -7.6159e-01, -0.0000e+00,  0.0000e+00,
           7.6159e-01,  0.0000e+00,  0.0000e+00, -7.6159e-01, -0.0000e+00,
           7.6159e-01, -5.4651e-44,  0.0000e+00,  9.9056e-25, -3.7046e-29,
          -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
           5.7992e-09, -2.5155e-08, -1.7199e-27,  0.0000e+00,  7.6159e-01,
           0.0000e+00,  2.0547e-14,  0.0000e+00, -0.0000e+00, -7.6159e-01,
           7.6159e-01,  0.0000e+00,  7.6159e-01,  0.0000e+00, -5.4255e-26,
          -4.9817e-36,  0.0000e+00, -1.2055e-10,  0.0000e+00,  0.0000e+00,
           7.6159e-01, -3.0385e-36, -7.6159e-01,  0.0000e+00,  0.0000e+00,
           7.6159e-01,  7.6159e-01,  9.4494e-19,  7.6159e-01, -2.2381e-18,
          -0.0000e+00,  0.0000e+00, -0.0000e+00,  7.6159e-01,  0.0000e+00,
           7.6159e-01, -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,
           7.6159e-01,  0.0000e+00, -0.0000e+00, -0.0000e+00,  0.0000e+00,
          -7.6155e-01,  0.0000e+00,  1.2784e-33, -1.3761e-19, -7.6159e-01,
           0.0000e+00, -1.8002e-08,  7.6159e-01,  0.0000e+00, -0.0000e+00,
          -0.0000e+00,  7.6159e-01, -1.2328e-15,  0.0000e+00, -7.6159e-01,
           0.0000e+00,  2.8026e-45, -7.6159e-01, -7.6159e-01,  7.6159e-01,
          -0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6159e-01,  0.0000e+00,
           7.6159e-01,  7.6159e-01,  7.6159e-01,  1.3698e-36, -0.0000e+00,
           2.6667e-18,  7.6159e-01, -7.6159e-01,  0.0000e+00, -2.3028e-19,
           7.6159e-01,  7.6159e-01,  7.6144e-01,  0.0000e+00, -3.0206e-23,
           7.6159e-01,  0.0000e+00,  7.6159e-01,  0.0000e+00, -0.0000e+00,
           0.0000e+00,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,
           0.0000e+00,  5.4651e-44, -2.5611e-11,  7.6159e-01,  7.6159e-01,
          -8.1393e-30,  1.4166e-17, -7.6159e-01,  0.0000e+00,  0.0000e+00,
          -2.9670e-26,  0.0000e+00,  0.0000e+00, -1.1600e-10,  7.6159e-01,
           0.0000e+00, -7.6159e-01, -8.2576e-26,  2.8384e-19,  7.6159e-01,
           0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
           1.8821e-19,  1.5063e-40, -7.6159e-01,  6.0221e-24,  0.0000e+00,
           0.0000e+00, -7.6156e-01, -2.3881e-04,  0.0000e+00,  2.9917e-34,
           0.0000e+00, -0.0000e+00, -7.6159e-01,  7.6159e-01, -5.8579e-18,
          -3.5826e-25,  9.7160e-14, -0.0000e+00, -7.6159e-01,  0.0000e+00,
           1.9411e-28, -1.8063e-30, -7.0389e-01,  0.0000e+00, -2.9772e-04,
          -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,  3.3789e-38,
          -7.6159e-01,  7.6159e-01, -5.1990e-07,  0.0000e+00,  7.6159e-01,
          -7.6159e-01, -7.6159e-01,  0.0000e+00, -7.6159e-01,  4.5837e-20,
           1.7211e-38,  7.6159e-01, -1.3623e-22,  0.0000e+00,  0.0000e+00,
           7.6159e-01,  0.0000e+00,  2.7991e-12,  0.0000e+00,  0.0000e+00,
          -7.6159e-01,  0.0000e+00,  0.0000e+00,  7.6159e-01, -9.9543e-09,
           0.0000e+00,  9.7930e-38,  0.0000e+00,  0.0000e+00,  0.0000e+00],
         [-7.6159e-01,  0.0000e+00, -7.6159e-01, -0.0000e+00, -0.0000e+00,
           4.6241e-19,  0.0000e+00,  0.0000e+00, -9.6403e-01, -0.0000e+00,
           9.6403e-01, -2.4980e-25,  0.0000e+00,  0.0000e+00, -7.5917e-01,
          -0.0000e+00,  1.8222e-32, -0.0000e+00,  0.0000e+00,  0.0000e+00,
           1.5291e-20, -7.6159e-01, -7.6159e-01,  0.0000e+00,  9.6403e-01,
           0.0000e+00,  2.9688e-10,  0.0000e+00, -0.0000e+00, -7.6159e-01,
           7.6159e-01,  5.2822e-35,  9.6403e-01,  0.0000e+00, -0.0000e+00,
          -6.9588e-29,  1.3366e-33, -2.6098e-12,  3.6504e-34,  5.0461e-37,
           7.6159e-01,  9.2886e-26, -7.6159e-01,  0.0000e+00,  0.0000e+00,
           7.6159e-01, -7.6159e-01,  7.6159e-01,  7.6159e-01, -3.4814e-34,
          -0.0000e+00,  0.0000e+00, -8.4637e-32,  2.6724e-08,  0.0000e+00,
           9.6403e-01, -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,
           9.6403e-01,  1.1798e-22,  6.4909e-32, -4.5410e-39,  0.0000e+00,
          -1.2136e-06, -1.9081e-30,  1.3084e-33, -8.4318e-32, -7.6159e-01,
          -1.8859e-34, -8.0370e-23,  7.6159e-01,  0.0000e+00, -0.0000e+00,
          -0.0000e+00,  9.6403e-01, -1.2328e-15,  0.0000e+00, -7.6159e-01,
           0.0000e+00,  0.0000e+00, -9.6403e-01, -7.0742e-01,  7.6159e-01,
          -0.0000e+00,  0.0000e+00,  0.0000e+00, -9.6403e-01,  0.0000e+00,
           9.6403e-01,  7.6159e-01,  9.6403e-01, -1.2280e-35, -0.0000e+00,
           6.0540e-36,  7.6159e-01, -7.6159e-01,  0.0000e+00,  2.7358e-26,
           7.6159e-01,  9.6403e-01,  9.6403e-01,  0.0000e+00,  1.2974e-32,
           9.6403e-01, -3.2921e-32,  9.6403e-01,  0.0000e+00, -3.9506e-15,
           0.0000e+00,  6.2820e-37, -7.6159e-01,  0.0000e+00,  0.0000e+00,
          -8.3345e-31, -3.8139e-28, -7.6159e-01,  9.6403e-01,  7.6159e-01,
          -1.1661e-17,  7.2913e-19, -7.6159e-01,  0.0000e+00,  0.0000e+00,
          -3.8827e-26,  0.0000e+00,  1.1380e-24, -1.7788e-28,  9.6403e-01,
          -0.0000e+00, -9.6403e-01,  1.4293e-25,  0.0000e+00,  7.6159e-01,
          -0.0000e+00, -2.8838e-32,  0.0000e+00, -0.0000e+00,  0.0000e+00,
           9.2322e-08,  0.0000e+00, -7.6159e-01,  2.2296e-17,  0.0000e+00,
           9.8448e-02, -1.1141e-03, -9.6403e-01, -1.8773e-41,  2.9938e-34,
           0.0000e+00, -3.0624e-31, -9.6403e-01,  7.6159e-01, -5.0198e-16,
          -1.1901e-26,  7.6159e-01, -0.0000e+00, -7.6159e-01, -6.4117e-26,
           1.9411e-28,  3.2427e-09, -2.1139e-17,  0.0000e+00, -2.9772e-04,
          -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,  1.1314e-12,
          -7.6159e-01,  3.2555e-02, -1.6039e-32,  0.0000e+00,  7.6159e-01,
          -7.6159e-01, -3.6908e-03,  0.0000e+00, -7.6159e-01,  1.9923e-27,
           1.4164e-31,  7.6159e-01, -2.6957e-35,  0.0000e+00,  0.0000e+00,
           7.3659e-15,  1.1691e-35,  1.2354e-08, -0.0000e+00,  0.0000e+00,
          -9.6403e-01,  0.0000e+00, -5.5543e-24,  9.6403e-01,  1.0291e-26,
           0.0000e+00,  4.5269e-37,  0.0000e+00, -1.3018e-23,  0.0000e+00]]],
       device='cuda:0', grad_fn=<CudnnRnnBackward>)
Esta salida se lo paso a una capa Lineal que mapea cada hidden state a un vetor de tama√±o 3
Salida Lineal: 
tensor([[[-19.1786,   9.8063, -26.2573],
         [-18.9755,   9.9838, -18.4642]]], device='cuda:0',
       grad_fn=<AddBackward0>)
Luego aplico Softmax para clasificacion
Resultado: 
tensor([[[2.5824e-13, 1.0000e+00, 2.1766e-16],
         [2.6495e-13, 1.0000e+00, 4.4178e-13]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
Luego transpongo para que sea corde a lo que asume la loss function de pytorch
tensor([[[2.5824e-13, 2.6495e-13],
         [1.0000e+00, 1.0000e+00],
         [2.1766e-16, 4.4178e-13]]], device='cuda:0',
       grad_fn=<TransposeBackward0>)
EJEMPLO: 
{'lxmertout': tensor([[[ 0.9197, -0.8722,  0.3635,  ..., -0.8474, -0.9470, -0.9191],
         [ 0.9048, -0.8086,  0.0401,  ..., -0.8768, -0.8679, -0.9370]]]), 'sizes': tensor([2]), 'answers': tensor([[1, 1]]), 'game_ids': tensor([2417]), 'raw_q': [['is it a person?'], ['in the foreground?']], 'qids': [tensor([4968]), tensor([4971])]}
torch.Size([1, 2, 768])
ANSWERS: 

tensor([[1, 1]])
Entrada: 
tensor([[[ 0.9197, -0.8722,  0.3635,  ..., -0.8474, -0.9470, -0.9191],
         [ 0.9048, -0.8086,  0.0401,  ..., -0.8768, -0.8679, -0.9370]]],
       device='cuda:0')
Se lo damos de comer a la LSTM
Output LSTM, para cada encoding tenemos su hidden state: 
tensor([[[-7.6159e-01,  0.0000e+00, -7.6159e-01, -0.0000e+00,  0.0000e+00,
           2.9763e-04,  0.0000e+00,  0.0000e+00, -7.6159e-01, -0.0000e+00,
           7.6159e-01, -2.0013e-41,  0.0000e+00,  3.6237e-26, -3.9284e-28,
          -0.0000e+00, -3.0417e-37, -0.0000e+00,  0.0000e+00,  0.0000e+00,
           3.0360e-09, -7.9545e-12, -3.6183e-17,  0.0000e+00,  7.6159e-01,
           0.0000e+00,  2.4024e-24,  0.0000e+00, -0.0000e+00, -7.6159e-01,
           7.6159e-01,  0.0000e+00,  7.6159e-01,  0.0000e+00, -5.5943e-32,
          -4.7871e-38,  0.0000e+00, -1.3846e-21,  0.0000e+00,  0.0000e+00,
           7.6159e-01, -6.4975e-34, -7.6143e-01,  0.0000e+00,  0.0000e+00,
           7.6159e-01,  7.6159e-01,  3.9965e-14,  7.6159e-01, -2.3126e-19,
          -0.0000e+00,  0.0000e+00,  0.0000e+00,  7.6159e-01,  0.0000e+00,
           7.6159e-01, -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,
           7.6159e-01,  0.0000e+00, -0.0000e+00,  0.0000e+00,  0.0000e+00,
          -7.6153e-01,  0.0000e+00,  5.7306e-31, -3.0161e-22, -7.6159e-01,
           0.0000e+00, -3.8272e-11,  7.6159e-01,  0.0000e+00, -6.9266e-39,
          -0.0000e+00,  7.6159e-01, -1.2501e-16,  0.0000e+00, -7.6159e-01,
           0.0000e+00,  2.2421e-44, -7.6159e-01, -7.6159e-01,  7.6159e-01,
          -0.0000e+00,  0.0000e+00,  0.0000e+00, -7.6159e-01,  0.0000e+00,
           7.6159e-01,  7.6159e-01,  7.6159e-01,  0.0000e+00, -0.0000e+00,
           2.3398e-26,  7.6159e-01, -7.6159e-01,  0.0000e+00, -5.6728e-23,
           7.6159e-01,  7.6159e-01,  3.9362e-01,  0.0000e+00, -2.1963e-24,
           7.6159e-01,  0.0000e+00,  7.6159e-01,  0.0000e+00, -0.0000e+00,
           0.0000e+00,  0.0000e+00, -7.6159e-01,  0.0000e+00,  0.0000e+00,
           0.0000e+00,  1.4475e-42, -1.3131e-05,  7.6159e-01,  7.6159e-01,
          -4.1547e-35,  5.0180e-17, -7.6159e-01,  0.0000e+00,  0.0000e+00,
          -1.2982e-24,  0.0000e+00,  0.0000e+00, -5.2225e-21,  7.6159e-01,
           0.0000e+00, -7.6159e-01, -3.4373e-36,  2.2281e-21,  7.6159e-01,
           0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
           1.4888e-13,  7.8570e-38, -7.6159e-01,  8.6891e-26,  0.0000e+00,
           0.0000e+00, -7.6144e-01, -3.1205e-06, -0.0000e+00,  0.0000e+00,
           0.0000e+00, -0.0000e+00, -7.6159e-01,  7.6159e-01, -8.1887e-19,
          -1.9364e-29,  7.1344e-09, -0.0000e+00, -7.6159e-01,  1.8950e-38,
           4.9162e-25, -2.3388e-31, -4.7817e-08,  0.0000e+00, -3.2131e-02,
          -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,  4.2433e-36,
          -7.6159e-01,  7.6159e-01, -5.3644e-13,  0.0000e+00,  7.6159e-01,
          -7.6159e-01, -7.6159e-01,  0.0000e+00, -7.6159e-01,  5.8578e-24,
           0.0000e+00,  7.6159e-01, -1.6848e-25,  0.0000e+00,  0.0000e+00,
           7.6159e-01,  0.0000e+00,  3.3820e-17,  0.0000e+00,  0.0000e+00,
          -7.6159e-01,  0.0000e+00,  0.0000e+00,  7.6159e-01, -1.0523e-20,
           0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],
         [-7.6159e-01,  0.0000e+00, -7.6159e-01, -0.0000e+00,  0.0000e+00,
           2.3147e-17,  0.0000e+00,  0.0000e+00, -9.6403e-01,  1.0075e-30,
           9.6403e-01, -1.8983e-25,  0.0000e+00,  1.3233e-34, -1.4455e-19,
          -0.0000e+00,  5.3177e-22, -1.1210e-44,  0.0000e+00,  0.0000e+00,
           2.9702e-16, -1.1867e-15, -7.6159e-01,  0.0000e+00,  9.6403e-01,
           0.0000e+00,  0.0000e+00,  0.0000e+00, -0.0000e+00, -7.6159e-01,
           7.6159e-01,  1.5238e-26,  9.6403e-01,  0.0000e+00, -0.0000e+00,
          -4.8954e-35,  0.0000e+00, -1.9921e-31,  0.0000e+00, -0.0000e+00,
           7.6159e-01, -1.3880e-24, -7.6143e-01,  0.0000e+00,  0.0000e+00,
           7.6159e-01, -7.6159e-01,  7.6159e-01,  8.7368e-15, -2.0919e-25,
          -0.0000e+00,  0.0000e+00,  8.4078e-45,  9.4447e-01,  0.0000e+00,
           9.6403e-01, -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,
           9.6403e-01,  0.0000e+00,  0.0000e+00, -2.2752e-38,  0.0000e+00,
          -7.6159e-01,  0.0000e+00,  8.4548e-27, -4.1807e-32, -7.6158e-01,
          -2.8903e-25, -8.5655e-25,  7.6159e-01,  0.0000e+00, -2.5349e-37,
          -0.0000e+00,  9.6403e-01, -6.8207e-16, -0.0000e+00, -7.6159e-01,
           0.0000e+00,  0.0000e+00, -9.6403e-01, -3.4403e-13,  7.6159e-01,
          -0.0000e+00,  0.0000e+00,  0.0000e+00, -9.6403e-01,  0.0000e+00,
           9.6403e-01,  7.6159e-01,  9.6403e-01,  0.0000e+00, -0.0000e+00,
           0.0000e+00,  7.6159e-01, -7.6159e-01,  0.0000e+00,  4.9068e-34,
           7.6159e-01,  9.6403e-01,  1.3783e-05,  0.0000e+00,  2.8160e-31,
           9.6403e-01, -6.8518e-20,  9.6403e-01,  0.0000e+00, -9.8527e-33,
           0.0000e+00,  4.1628e-30, -7.6159e-01,  0.0000e+00,  0.0000e+00,
           1.3694e-32, -1.1128e-29, -7.6159e-01,  9.6403e-01,  7.6159e-01,
          -2.3256e-36,  6.7704e-25, -7.6159e-01,  0.0000e+00,  0.0000e+00,
          -1.3132e-20,  0.0000e+00,  0.0000e+00, -0.0000e+00,  9.6403e-01,
           0.0000e+00, -6.2019e-12, -0.0000e+00,  2.7301e-27,  7.6159e-01,
          -0.0000e+00, -0.0000e+00,  0.0000e+00, -0.0000e+00,  0.0000e+00,
           7.6159e-01,  0.0000e+00, -7.6159e-01,  4.7694e-31,  0.0000e+00,
           1.1086e-24, -8.2874e-10, -1.1955e-08, -3.9052e-40,  0.0000e+00,
           0.0000e+00, -0.0000e+00, -9.6403e-01,  7.6159e-01, -6.8349e-19,
          -1.1881e-38,  7.8858e-01, -0.0000e+00, -7.6159e-01, -1.6398e-18,
           2.2523e-22,  4.6146e-20, -1.7201e-33,  0.0000e+00, -3.2445e-02,
          -7.6159e-01,  0.0000e+00,  0.0000e+00, -0.0000e+00,  3.0256e-26,
          -7.6159e-01,  7.6159e-01, -8.1460e-34,  0.0000e+00,  7.6159e-01,
          -7.6159e-01, -2.5793e-02, -1.4735e-26, -7.6159e-01,  1.1251e-31,
           0.0000e+00,  7.6159e-01, -1.9892e-27,  0.0000e+00,  0.0000e+00,
           9.6403e-01,  0.0000e+00,  1.7329e-08,  0.0000e+00,  0.0000e+00,
          -9.6403e-01,  0.0000e+00,  0.0000e+00,  9.6403e-01, -0.0000e+00,
           0.0000e+00,  0.0000e+00,  0.0000e+00, -1.3250e-24,  0.0000e+00]]],
       device='cuda:0', grad_fn=<CudnnRnnBackward>)
Esta salida se lo paso a una capa Lineal que mapea cada hidden state a un vetor de tama√±o 3
Salida Lineal: 
tensor([[[-18.9567,   9.4810, -23.7630],
         [-17.6317,  12.8823, -25.3276]]], device='cuda:0',
       grad_fn=<AddBackward0>)
Luego aplico Softmax para clasificacion
Resultado: 
tensor([[[4.4634e-13, 1.0000e+00, 3.6501e-15],
         [5.5966e-14, 1.0000e+00, 2.5448e-17]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
Luego transpongo para que sea corde a lo que asume la loss function de pytorch
tensor([[[4.4634e-13, 5.5966e-14],
         [1.0000e+00, 1.0000e+00],
         [3.6501e-15, 2.5448e-17]]], device='cuda:0',
       grad_fn=<TransposeBackward0>)
EJEMPLO: 
{'lxmertout': tensor([[[-0.4959,  0.6438,  0.8017,  ...,  0.1347,  0.9791,  0.1273],
         [-0.6829, -0.2396,  0.2795,  ...,  0.1564,  0.8975, -0.6973],
         [-0.4376,  0.3767,  0.9129,  ..., -0.7690,  0.9412,  0.3005],
         ...,
         [ 0.7655, -0.8246,  0.2482,  ...,  0.2579,  0.8892, -0.8668],
         [ 0.0513, -0.2959,  0.7402,  ..., -0.2402,  0.9980, -0.2957],
         [ 0.6901, -0.8795,  0.7420,  ..., -0.2917,  0.9097, -0.9218]]]), 'sizes': tensor([8]), 'answers': tensor([[0, 0, 0, 0, 0, 1, 0, 1]]), 'game_ids': tensor([2421]), 'raw_q': [['is it furniture?'], ['is it white?'], ['is it on the wall?'], ['is it on the ground?'], ['is it light up?'], ['is it on the table?'], ['is it on the close table?'], ['is it on the left on the far table?']], 'qids': [tensor([4973]), tensor([4982]), tensor([4991]), tensor([4995]), tensor([5000]), tensor([5008]), tensor([5020]), tensor([5037])]}
torch.Size([1, 8, 768])
ANSWERS: 

tensor([[0, 0, 0, 0, 0, 1, 0, 1]])
Entrada: 
tensor([[[-0.4959,  0.6438,  0.8017,  ...,  0.1347,  0.9791,  0.1273],
         [-0.6829, -0.2396,  0.2795,  ...,  0.1564,  0.8975, -0.6973],
         [-0.4376,  0.3767,  0.9129,  ..., -0.7690,  0.9412,  0.3005],
         ...,
         [ 0.7655, -0.8246,  0.2482,  ...,  0.2579,  0.8892, -0.8668],
         [ 0.0513, -0.2959,  0.7402,  ..., -0.2402,  0.9980, -0.2957],
         [ 0.6901, -0.8795,  0.7420,  ..., -0.2917,  0.9097, -0.9218]]],
       device='cuda:0')
Se lo damos de comer a la LSTM
Output LSTM, para cada encoding tenemos su hidden state: 
tensor([[[-7.6159e-01,  7.6159e-01,  0.0000e+00,  ...,  0.0000e+00,
           0.0000e+00,  5.6391e-02],
         [-7.6159e-01,  5.5258e-01,  7.6159e-01,  ...,  3.3226e-30,
          -2.3060e-21,  5.6391e-02],
         [-7.6159e-01,  7.6159e-01,  1.7331e-12,  ...,  3.3226e-30,
          -2.3060e-21, -7.3685e-01],
         ...,
         [-7.6159e-01,  5.4651e-44, -7.6159e-01,  ...,  6.9191e-37,
          -2.3060e-21, -7.3699e-01],
         [-7.6159e-01,  7.6159e-01, -7.6159e-01,  ...,  3.3176e-25,
          -2.3060e-21, -7.3699e-01],
         [-7.6159e-01,  0.0000e+00, -7.6159e-01,  ...,  1.3351e-37,
          -2.3060e-21, -7.3699e-01]]], device='cuda:0',
       grad_fn=<CudnnRnnBackward>)
Esta salida se lo paso a una capa Lineal que mapea cada hidden state a un vetor de tama√±o 3
Salida Lineal: 
tensor([[[ -0.9786, -14.5188, -17.1729],
         [ -2.3548,  -8.7793, -11.2085],
         [ -1.5065, -16.0900, -18.1005],
         [  0.6762, -11.6161, -14.5875],
         [  0.8266, -12.9632, -13.7702],
         [-22.4764,   9.4322, -15.1311],
         [ -0.5801,  -9.2713, -17.1638],
         [-24.2341,  13.5005, -20.8345]]], device='cuda:0',
       grad_fn=<AddBackward0>)
Luego aplico Softmax para clasificacion
Resultado: 
tensor([[[1.0000e+00, 1.3169e-06, 9.2664e-08],
         [9.9824e-01, 1.6186e-03, 1.4260e-04],
         [1.0000e+00, 4.6397e-07, 6.2135e-08],
         [1.0000e+00, 4.5866e-06, 2.3498e-07],
         [1.0000e+00, 1.0261e-06, 4.5784e-07],
         [1.3876e-14, 1.0000e+00, 2.1493e-11],
         [9.9983e-01, 1.6803e-04, 6.2765e-08],
         [4.0932e-17, 1.0000e+00, 1.2259e-15]]], device='cuda:0',
       grad_fn=<SoftmaxBackward>)
Luego transpongo para que sea corde a lo que asume la loss function de pytorch
tensor([[[1.0000e+00, 9.9824e-01, 1.0000e+00, 1.0000e+00, 1.0000e+00,
          1.3876e-14, 9.9983e-01, 4.0932e-17],
         [1.3169e-06, 1.6186e-03, 4.6397e-07, 4.5866e-06, 1.0261e-06,
          1.0000e+00, 1.6803e-04, 1.0000e+00],
         [9.2664e-08, 1.4260e-04, 6.2135e-08, 2.3498e-07, 4.5784e-07,
          2.1493e-11, 6.2765e-08, 1.2259e-15]]], device='cuda:0',
       grad_fn=<TransposeBackward0>)
